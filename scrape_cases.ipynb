{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import PyPDF2\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate constants \n",
    "\n",
    "URL_BASE = \"https://www.ca9.uscourts.gov\"\n",
    "FILE_NAME_BASE = \"./opinions-\"\n",
    "RELEVANT_COLUMNS = ['case_name', 'case_num', 'case_origin', 'judge', 'case_type', 'short_date', 'file_name']\n",
    "SCRAPE_START_DATE = '2018-11-1'\n",
    "SCRAPE_END_DATE = '2023-11-1'\n",
    "SUMMARIZATION_PROMPT = \"\"\"TASK: Summarize the following legal opinion document in 1000 words or less. In the summary, include the case name and go into detail about the opinion of the authoring judge and any precedent used or established to support the final decision.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read opinion files from https://cdn.ca9.uscourts.gov/assets/Public-Data.pdf\n",
    "\n",
    "jsons = []\n",
    "for i in range(1, 5):\n",
    "    file_name = FILE_NAME_BASE + str(i)\n",
    "    with open(file_name) as f:\n",
    "        for line in f.readlines():\n",
    "            line_json = json.loads(line)\n",
    "            jsons.append(line_json[\"Item\"])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store relevant information in DataFrame\n",
    "\n",
    "df = pd.DataFrame.from_records(jsons)\n",
    "df = df[RELEVANT_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format columns and limit the time range of cases\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "for col in df.columns:\n",
    "    df_new[col] = df[col].apply(lambda x: str(x[\"S\"]) if type(x) == dict else x)\n",
    "\n",
    "df = df_new\n",
    "\n",
    "df['date'] = pd.to_datetime(df['short_date'])  \n",
    "\n",
    "mask = (df['date'] > SCRAPE_START_DATE) & (df['date'] <= SCRAPE_END_DATE)\n",
    "df = df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPEN_AI_KEY\"))\n",
    "\n",
    "# Generate a 1000 word summary for the given case.\n",
    "def generate_case_summary(case_name, opinion_text):\n",
    "    curr_prompt = SUMMARIZATION_PROMPT + \"**CASE NAME**: \" + case_name + \"\\n**OPINION TEXT**: \" + opinion_text\n",
    "\n",
    "    curr_prompt_token_count = len(curr_prompt) * 0.25\n",
    "    if curr_prompt_token_count < 16000:\n",
    "        messages = []\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": curr_prompt})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        overall_summary = response.choices[0].message.content\n",
    "\n",
    "        return overall_summary      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and store the opinion text and their summaries\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(index)\n",
    "    case_name = row[\"file_name\"]\n",
    "    case_url = URL_BASE + case_name\n",
    "    r = requests.get(case_url)\n",
    "    if r.status_code == 200:\n",
    "        f = io.BytesIO(r.content)\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        pdf_content = \"\"\n",
    "        for page in reader.pages:\n",
    "            pdf_content += page.extract_text() + \"\\n\\n\"\n",
    "        df.loc[index, 'opinion_text'] = pdf_content\n",
    "        df.loc[index, 'case_summary'] = generate_case_summary(row[\"case_name\"], pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cases in a pickle file.\n",
    "\n",
    "df.to_pickle(\"./cases.pkl\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
