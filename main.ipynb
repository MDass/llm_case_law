{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install trulens-eval==0.12.0 llama_index==0.8.4 pymilvus==2.3.0 nltk==3.8.1 html2text==2020.1.16 tenacity==8.2.3 --quiet\n",
    "# %pip install wikipedia transformers sentence-transformers --quiet\n",
    "# %pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from llama_index import Document, VectorStoreIndex, ServiceContext\n",
    "import pandas as pd\n",
    "from llama_index.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from trulens_eval import Tru, TruLlama, Feedback\n",
    "from trulens_eval.feedback import Groundedness\n",
    "import numpy as np\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prompts\n",
    "PROMPTS = [\n",
    "    \"I am arguing a case in front of Judge Callahan soon. My client believes his first and fifth amendment rights were violated. What arguments about this issue has Judge Callahan found compelling in other cases?\"\n",
    "    # \"What are the primary issues and concerns that the court has dealt with when it comes to cases brought forth by tribal nations?\"\n",
    "    # \"What are the prevailing opinions and thoughts regarding the possession of a firearm in the Missoula district court?\"\n",
    "    # \"My client crossed state lines to perform an illegal action. Based on USA vs. Michael Pepe, what are some arguments I should make?\"\n",
    "    # \"What do past cases in this district indicate about the prevailing belief/precedent in regards to the death penalty?\"\n",
    "    # \"I plan to challenge the constitutionality of a current law under the Second Amendment. What arguments have been effective or convincing on this issue before?\"\n",
    "    # \"I am arguing a case against a technology company. What do past cases indicate regarding key concerns when arguing against a company?\"\n",
    "    # \"I am arguing a free speech/public fora case. My client tried distributing tokens in a Free Speech Zone. What have past cases said about this? Mention the names of these cases specifically.\"\n",
    "    # \"I am arguing in front of Judge Sanchez soon. How has he previously voted in immigration appeal cases?\"\n",
    "]\n",
    "\n",
    "generated_responses = []\n",
    "\n",
    "PER_CASE_PROMPT_ADDENDUM = \"\"\"Please mention the name and date of any relevant cases in your response. \n",
    "                If no relevant information is found in the context, set [response] below to \"NO RESPONSE\".\n",
    "                    Format responses, as so:\n",
    "                            RESPONSE: [response]\n",
    "                            SOURCE: [case_name of the document the response was formulated from]\n",
    "                    Also, the audience consists of lawyers, so use legal jargon and a formal tone. Your final answer should be under 3000 characters.\"\"\"\n",
    "SYNTHESIZED_PROMPT_ADDENDUM = \"In your answer, focus on the reasoning and principles used in prior cases. Also mention how cases cited are relevant to the case mentioned in the user's query. Integrate the name of the sources used into your response.\"\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DOCUMENTS FOR EACH CASE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata for each case.\n",
    "\n",
    "cases_df = pd.read_pickle(\"./cases.pkl\")\n",
    "documents = []\n",
    "for index, row in cases_df.iterrows():\n",
    "    doc_text = \"\"\"\n",
    "    CASE NAME: {}\n",
    "    JUDGE: {}\n",
    "    CASE SUMMARY: {}    \n",
    "    \"\"\".format(row[\"case_name\"], row[\"judge\"], row[\"case_summary\"])\n",
    "    doc = Document(\n",
    "        text=doc_text,\n",
    "        metadata={\n",
    "            'case_name':row[\"case name\"],\n",
    "            'case_number':row[\"case number\"],\n",
    "            'case_origin':row[\"case origin\"],\n",
    "            'judge':row[\"authoring judge\"],\n",
    "            'case_type':row[\"case type\"],\n",
    "            'date':row[\"date filed\"]\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZE VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "embed_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize MilvusVectorStore\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    index_params= {\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\":{\"nlist\":1024, \"nprobe\":100}\n",
    "    },\n",
    "    search_params={},\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model = embed_model, llm = llm)\n",
    "index = VectorStoreIndex.from_documents(documents,\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TruLens\n",
    "tru = Tru()\n",
    "\n",
    "# Initialize OpenAI-based feedback function collection class\n",
    "openai_gpt35 = feedback.OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize groundedness class for the groundedness metric\n",
    "grounded = Groundedness(groundedness_provider=openai_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_answer_relevance = Feedback(openai_gpt35.relevance_with_cot_reasons, name = \"Answer Relevance\").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_groundedness = Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\").on(\n",
    "    TruLlama.select_source_nodes().node.text # this line grabs the context that was supplied with the query\n",
    ").on_output().aggregate(grounded.grounded_statements_aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_context_relevance = Feedback(openai_gpt35.qs_relevance_with_cot_reasons, name = \"Context Relevance\").on_input().on(TruLlama.select_source_nodes().node.text).aggregate(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOOSE TOP K RELEVANT CASE SUMMARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k = 5)\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "    \n",
    "# Initialize a TruLlama wrapper to connect evaluation metrics with the query engine\n",
    "tru_query_engine = TruLlama(query_engine,\n",
    "                    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    "                    metadata={\n",
    "                        'index_param':'',\n",
    "                        'embed_model':\"top k cases\",\n",
    "                        'top_k':5,\n",
    "                        'chunk_size':200\n",
    "                        })\n",
    "    \n",
    "@retry(stop=stop_after_attempt(10), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "\n",
    "def call_tru_retriever_engine(prompt):\n",
    "    return retriever.retrieve(prompt)\n",
    "\n",
    "prompt_to_cases_dict = {}\n",
    "for prompt in PROMPTS:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    # print(f\"Response: {call_tru_query_engine(prompt)}\\n\")\n",
    "    prompt_to_cases_dict[prompt] = []\n",
    "    nodes = call_tru_retriever_engine(prompt)\n",
    "    for node in nodes:\n",
    "        print(\"case name: {}, score: {}\".format(node.node.metadata[\"case_name\"], str(node.score)))\n",
    "        if node.score < 0.4:\n",
    "            prompt_to_cases_dict[prompt].append(node.node.metadata[\"case_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETREIEVE FULL OPINION TEXT AND GENERATE RESPONSE FOR EACH TOP K CASE\n",
    "- Place each case in separate vector db\n",
    "- Use rag to generate response to prompt from only this vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(10), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def call_tru_query_engine(tru_query_engine, prompt):\n",
    "        # we now send the prompt through the TruLlama-wrapped query engine\n",
    "    return tru_query_engine.query(prompt)\n",
    "\n",
    "responses = {}\n",
    "for prompt in prompt_to_cases_dict:\n",
    "    cases = prompt_to_cases_dict[prompt]\n",
    "    responses[prompt] = []\n",
    "    for case in cases:\n",
    "        row = cases_df.loc[cases_df[\"case name\"] == case].iloc[0]\n",
    "\n",
    "        opinion = row[\"opinion text\"]\n",
    "       \n",
    "        # create document with full opinion text and add to vector store\n",
    "        doc = Document(\n",
    "                text=opinion,\n",
    "                metadata={\n",
    "                    'case_name':row[\"case name\"],\n",
    "                    'case_number':row[\"case number\"],\n",
    "                    'case_origin':row[\"case origin\"],\n",
    "                    'judge':row[\"authoring judge\"],\n",
    "                    'case_type':row[\"case type\"],\n",
    "                    'date':row[\"date filed\"]\n",
    "                }\n",
    "        )\n",
    "        vector_store_two = MilvusVectorStore(\n",
    "            index_params= {\n",
    "                \"index_type\": \"IVF_FLAT\",\n",
    "                \"metric_type\": \"L2\",\n",
    "                \"params\":{\"nlist\":1024, \"nprobe\":100}\n",
    "            },\n",
    "            search_params={},\n",
    "            overwrite=True)\n",
    "\n",
    "        storage_context = StorageContext.from_defaults(vector_store = vector_store_two)\n",
    "        service_context = ServiceContext.from_defaults(embed_model = embed_model, llm = llm)\n",
    "        index = VectorStoreIndex.from_documents([doc],\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context)\n",
    "        query_engine = index.as_query_engine(similarity_top_k = 5)\n",
    "\n",
    "        # query rag to get the response to the prompt per case\n",
    "        tru_query_engine = TruLlama(query_engine,\n",
    "                        feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    "                        metadata={\n",
    "                            'index_param':f\"Prompt index: {PROMPTS.index(prompt)}, case name: {row['case name']}\",\n",
    "                            'embed_model':\"per case response\",\n",
    "                            'top_k':5,\n",
    "                            'chunk_size':1000\n",
    "                            })\n",
    "        response = call_tru_query_engine(tru_query_engine, prompt + PER_CASE_PROMPT_ADDENDUM)\n",
    "        responses[prompt].append(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE RAG TO SYNTHESIZE 5 RESPONSES INTO 1 FINAL RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in responses:\n",
    "    vector_store_three = MilvusVectorStore(\n",
    "                index_params= {\n",
    "                    \"index_type\": \"IVF_FLAT\",\n",
    "                    \"metric_type\": \"L2\",\n",
    "                    \"params\":{\"nlist\":1024, \"nprobe\":100}\n",
    "                },\n",
    "                search_params={},\n",
    "                overwrite=True)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store = vector_store_three)\n",
    "    service_context = ServiceContext.from_defaults(embed_model = embed_model, llm = llm)\n",
    "    documents = []\n",
    "    for response in responses[prompt]:\n",
    "        if \"NO RESPONSE\" in str(response):\n",
    "            continue\n",
    "        doc = Document(\n",
    "                    text=str(response),\n",
    "            )\n",
    "        documents.append(doc)\n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                service_context=service_context,\n",
    "                storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine(similarity_top_k = 5)\n",
    "\n",
    "    tru_query_engine = TruLlama(query_engine,\n",
    "                            feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    "                            metadata={\n",
    "                                'index_param':'',\n",
    "                                'embed_model':\"synthesis\",\n",
    "                                'top_k':5,\n",
    "                                'chunk_size':3000\n",
    "                                })\n",
    "    query_response = call_tru_query_engine(tru_query_engine, prompt + SYNTHESIZED_PROMPT_ADDENDUM)\n",
    "    generated_responses.append(str(query_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRUERA EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in PROMPTS:\n",
    "    print(response)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PICKLE FOR BREADTH EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_to_summaries = {}\n",
    "\n",
    "for response in generated_responses:\n",
    "    ind = generated_responses.index(response)\n",
    "    prompt = generated_responses[ind]\n",
    "    response_to_summaries[str(response)] = [str(x) for x in responses[prompt]]\n",
    "\n",
    "with open(\"generated_responses.pickle\", 'wb') as handle:\n",
    "    pickle.dump(response_to_summaries, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
